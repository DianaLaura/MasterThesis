{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 4: DTA, linear regression and subwords**\n",
    "\n",
    "*Background*: Experiment 2 shows that linear regression with 215 features does not overfit anymore, but the model does not fit the beginning and the end of the timerange. Quadratic polynomial regression would be a possibility, but quadratic polynomial regression performs worse on both training and validation set than linear regression. Polynomial regression with a degree of 5 cannot be computed due to the vast number of polynomial features created by the model.\n",
    "\n",
    "*Goal*: Determine if it is possible to predict the year in which a text was written using regression.\n",
    "\n",
    "*Strategies*:\n",
    "\n",
    "- Use a BPE-transformer to train on subwords (Sennrich2016)\n",
    "\n",
    "*Relevance*:\n",
    "\n",
    "- If this experiment works, it is possible to estimate years for corpora that have NA's in this variable.\n",
    "- Subwords might increase the amount of generalization, and minimize the vocabulary used at the same time.\n",
    "\n",
    "*Success criteria*:\n",
    "\n",
    "- Consistent findings over training-, test- and validation set\n",
    "- predicted year is not more than ten years away from the true year\n",
    "\n",
    "*Corpora*:\n",
    "\n",
    "- DTA\n",
    "\n",
    "*Baseline to beat (Exp. 2)*:\n",
    "- Train MSE = 2259.8\n",
    "- Val MSE = 3202.51\n",
    "\n",
    "*Result*: - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dianaenggist/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/dianaenggist/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest , f_regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.utils\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from Selfwritten_modules.SubwordTransformer import SubwordTransformer\n",
    "\n",
    "import re\n",
    "\n",
    "import eli5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test bpe-algorithm on a small play-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "playset = pd.read_csv('/users/dianaenggist/Documents/Masterprojekt/testfile_deutsch_tokenized.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               Text\n",
      "0           0  [['Die', 'Pressekonferenz', 'ist', 'beendet', ...\n",
      "1           1  [['Strupler', ':', '«', 'Wir', 'müssen', 'zuer...\n",
      "2           2  [['Koch', ':', '«', 'Es', 'gibt', 'bereits', '...\n"
     ]
    }
   ],
   "source": [
    "print(playset)\n",
    "playset = playset['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build tokenizer that just substitutes '[' and ']' with ','\n",
    "def tokenizer_word(doc):\n",
    "    doc = re.sub('[(\\[+)|(\\]+)]', '', doc)\n",
    "    doc = re.split(',', doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = SubwordTransformer(tokenizer=tokenizer_word)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\ \\ '\n",
      " \n",
      "'\n"
     ]
    }
   ],
   "source": [
    "vocabulary = transformer.bpe(playset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {\"'Die'\": 1, \" 'Pressekonferenz'\": 1, \" 'ist'\": 1, \" 'beendet'\": 1, \" '.'\": 6, \" 'Vielen'\": 1, \" 'Dank'\": 1, \" 'für'\": 1, \" 'Ihre'\": 1, \" 'Aufmerksamkeit'\": 1, \" 'Weiter'\": 1, \" 'geht'\": 1, \" 'es'\": 1, \" 'um'\": 2, \" '15.30'\": 1, \" 'Uhr'\": 1, \" 'mit'\": 1, \" 'der'\": 1, \" 'Medienkonferenz'\": 1, \" 'des'\": 1, \" 'Kantons'\": 1, \" 'Graubünden'\": 1, \" 'Die'\": 1, \" 'Behörden'\": 1, \" 'nehmen'\": 1, \" 'Stellung'\": 1, \" 'zu'\": 2, \" 'den'\": 1, \" 'zwei'\": 1, \" 'bestätigten'\": 1, \" 'Coronavirus-Fällen'\": 1, \" 'im'\": 1, \" 'Kanton'\": 1, \"'Koch'\": 1, \" ':'\": 1, \" '«'\": 1, \" 'Es'\": 2, \" 'gibt'\": 1, \" 'bereits'\": 1, \" 'europaweit'\": 1, \" 'Aufrufe'\": 1, \" '\": 3, \"'\": 4, \" 'Forschung'\": 1, \" 'generieren'\": 1, \" 'wird'\": 1, \" 'schnell'\": 1, \" 'und'\": 2, \" 'stark'\": 1, \" 'an'\": 2, \" 'Impfstoffen'\": 1, \" 'Medikamenten'\": 1, \" 'geforscht.'\": 1, \" '»'\": 1, \" 'Zu'\": 1, \" 'sagen'\": 1, \" 'wie'\": 1, \" 'lange'\": 1, \" 'das'\": 1, \" 'dauere'\": 1, \" 'sei'\": 1, \" 'aber'\": 1, \" 'Spekulation'\": 1, ' ': 1})\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code example: https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le\n",
    "def features_to_names(features, feature_names):\n",
    "    features_selected = []\n",
    "\n",
    "    for bool, feature in zip(features, feature_names):\n",
    "        if bool:\n",
    "            features_selected.append(feature)\n",
    "    return features_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for assembling predictions in order to find out how features are weighted\n",
    "\n",
    "def collect_predictions(dataset, classifier,vectorizer, feature_names, pipeline):\n",
    "    predictions = eli5.explain_weights_df(classifier,vec=vectorizer, feature_names=feature_names)\n",
    "    \n",
    "    predictions = predictions.drop(['target'], axis=1)\n",
    "    \n",
    "    \n",
    "    predictions['YEAR'] = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    for instance in range (0, len(dataset)):\n",
    "        pred = eli5.explain_prediction_df(classifier, dataset[instance], vec=vectorizer, feature_names=feature_names)\n",
    "        source_text = pd.DataFrame([[dataset[instance]]])\n",
    "        year_pred = pipeline.predict(source_text[0])\n",
    "        pred['weight_value'] = pred['weight'] * pred['value']\n",
    "        pred['instance'] = instance\n",
    "        \n",
    "        \n",
    "        pred = pred.drop(['target','weight','value'], axis=1)\n",
    "        \n",
    "    \n",
    "        pred['YEAR'] = np.round(year_pred[0])\n",
    "    \n",
    "        predictions = pd.concat([predictions, pred])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.read_csv('/Volumes/Korpora/Train/DTA_train_tokenized.csv', sep=';')\n",
    "val_full = pd.read_csv('/Volumes/Korpora/Val/DTA_val_tokenized.csv', sep=';')\n",
    "#test_full = pd.read_csv('/Volumes/Korpora/Test/DTA_test_tokenized.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length train set: ',len(train_full))\n",
    "print('Length validation set: ', len(val_full))\n",
    "#print('Length test set: ', len(test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_full['Text']\n",
    "train_y = train_full['Publication_year']\n",
    "\n",
    "val_x = val_full['Text']\n",
    "val_y = val_full['Publication_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
