{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 1b: linear regression and selecting features that occur a certain amount of time across corpora in the same language**\n",
    "\n",
    "*Background*: Exp. 1b works well on all corpora.\n",
    "\n",
    "*Goal*: Watch the performance of linear regression on a validation- / test set that is in the same language as the train set, but not from the same corpus. This helps to determine if the algorithm generalizes well, and if the features it picks are really typical for a certain time period (cf. \n",
    "\n",
    "*Strategies*:\n",
    "- Train on features that occur a certain amount of time\n",
    "\n",
    "*Relevance*:\n",
    "- If this experiment works, it is possible to estimate years for corpora that have NA's in this variable.\n",
    "\n",
    "*Success criteria*:\n",
    "- Consistent findings over different corpora in the same language\n",
    "- predicted year is not more than ten years away from the true year\n",
    "\n",
    "*Corpora*:\n",
    "- DTA\n",
    "- CLMET\n",
    "- GERMANC\n",
    "- ARCHER\n",
    "\n",
    "*Result*: The classifier trained over DTA performs very well on the validation set from the GERMANC. It does not work well the other way round, which is unsurprising given the fact that GERMANC provides much less features than the larger DTA.\n",
    "\n",
    "With CLMET and ARCHER, the performance is not good. This can be due to the fact that ARCHER's parameters have a complete different ratio than the parameters of the other corpora.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "*MSE DTA Train*: 2259.8\n",
    "\n",
    "*MSE DTA Val*: 3202.51\n",
    "\n",
    "*MSE DTA Test*: 4504.35\n",
    "\n",
    "*MSE over GERMANC val*: 3700.60\n",
    "\n",
    "*MSE over GERMANC test*: 3565.05\n",
    "Setup: \n",
    "- features occur in about 89% of documents in the train set\n",
    "- number of features is 25% of the number of documents in the train set\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "*MSE CLMET Train*: 1346.26\n",
    "\n",
    "*MSE CLMET Val*: 2727.37\n",
    "\n",
    "*MSE CLMET Test*: 4315.18\n",
    "\n",
    "*MSE over ARCHER Val*: 10212.14\n",
    "\n",
    "*MSE over ARCHER Test*: 9784.28\n",
    "\n",
    "Setup:\n",
    "- features occur in about 89% of documents in the train set\n",
    "- number of features is 25% of the number of documents in the train set\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "*MSE ARCHER Train*: 3555.68\n",
    "\n",
    "*MSE ARCHER Val*: 4843.06\n",
    "\n",
    "*MSE ARCHER Test*: 4939.51\n",
    "\n",
    "*MSE over CLMET Val*: 6437126.77\n",
    "\n",
    "*MSE over CLMET Test*: 6689227.44\n",
    "\n",
    "Setup:\n",
    "- features occur in about 48% of documents in the train set\n",
    "- number of features is 15% of the number of documents in the train set\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "*MSE GERMANC Train*: 348.830\n",
    "\n",
    "*MSE GERMANC Val*: 398.94\n",
    "\n",
    "*MSE GERMANC Test*: 649.33\n",
    "\n",
    "*MSE over DTA val*: 8400009.85\n",
    "\n",
    "*MSE over DTA test*: 21896163.78\n",
    "\n",
    "Setup:\n",
    "- features occur in about 89% of documents in the train set\n",
    "- number of features is 25% of the number of documents in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest , f_regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.utils\n",
    "import re\n",
    "\n",
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code example: https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le\n",
    "def features_to_names(features, feature_names):\n",
    "    features_selected = []\n",
    "\n",
    "    for bool, feature in zip(features, feature_names):\n",
    "        if bool:\n",
    "            features_selected.append(feature)\n",
    "    return features_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build tokenizer that just substitutes '[' and ']' with ','\n",
    "def tokenizer_word(doc):\n",
    "    doc = re.sub('[(\\[+)|(\\]+)]', '', doc)\n",
    "    doc = re.split(',', doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for assembling predictions in order to find out how features are weighted\n",
    "\n",
    "def collect_predictions(dataset, classifier,vectorizer, feature_names, pipeline):\n",
    "    predictions = eli5.explain_weights_df(classifier,vec=vectorizer, feature_names=feature_names)\n",
    "    \n",
    "    predictions = predictions.drop(['target'], axis=1)\n",
    "    \n",
    "    \n",
    "    predictions['YEAR'] = 0\n",
    "    \n",
    "    indexes = dataset.index.values\n",
    "    \n",
    "    \n",
    "\n",
    "    for index in indexes:\n",
    "        \n",
    "        pred = eli5.explain_prediction_df(classifier, dataset[index], vec=vectorizer, feature_names=feature_names)\n",
    "        \n",
    "        source_text = pd.DataFrame([[dataset[index]]])\n",
    "        \n",
    "        year_pred = pipeline.predict(source_text[0])\n",
    "        pred['weight_value'] = pred['weight'] * pred['value']\n",
    "        pred['instance'] = index\n",
    "        \n",
    "        pred = pred.drop(['target','value'], axis=1)\n",
    "        \n",
    "    \n",
    "        pred['YEAR'] = np.round(year_pred[0])\n",
    "    \n",
    "        predictions = pd.concat([predictions, pred])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    return predictions.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTA_train_full = pd.read_csv('/Volumes/Korpora/Train/DTA_train_tokenized.csv', sep=';')\n",
    "DTA_val_full = pd.read_csv('/Volumes/Korpora/Val/DTA_val_tokenized.csv', sep=';')\n",
    "DTA_test_full = pd.read_csv('/Volumes/Korpora/Test/DTA_test_tokenized.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train set:  899\n",
      "Length validation set:  225\n",
      "Length test set:  281\n"
     ]
    }
   ],
   "source": [
    "print('Length train set: ',len(DTA_train_full))\n",
    "print('Length validation set: ', len(DTA_val_full))\n",
    "print('Length test set: ', len(DTA_test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTA_train_x = DTA_train_full['Text']\n",
    "DTA_train_y = DTA_train_full['Publication_year']\n",
    "\n",
    "DTA_val_x = DTA_val_full['Text']\n",
    "DTA_val_y = DTA_val_full['Publication_year']\n",
    "\n",
    "DTA_test_x = DTA_test_full['Text']\n",
    "DTA_test_y = DTA_test_full['Publication_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLMET_train_full = pd.read_csv('/Volumes/Korpora/Train/CLMET_train_tokenized.csv', sep=';')\n",
    "CLMET_val_full = pd.read_csv('/Volumes/Korpora/Val/CLMET_val_tokenized.csv', sep=';')\n",
    "CLMET_test_full = pd.read_csv('/Volumes/Korpora/Test/CLMET_test_tokenized.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with invalid data types\n",
    "CLMET_train_full = CLMET_train_full[CLMET_train_full.Year.str.len()== 4]\n",
    "CLMET_val_full = CLMET_val_full[CLMET_val_full.Year.str.len()== 4]\n",
    "CLMET_test_full = CLMET_test_full[CLMET_test_full.Year.str.len()== 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train set:  186\n",
      "Length validation set:  47\n",
      "Length test set:  60\n"
     ]
    }
   ],
   "source": [
    "print('Length train set: ',len(CLMET_train_full))\n",
    "print('Length validation set: ', len(CLMET_val_full))\n",
    "print('Length test set: ', len(CLMET_test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLMET_train_x = CLMET_train_full['Text']\n",
    "CLMET_train_y = CLMET_train_full['Year'].astype(int)\n",
    "\n",
    "CLMET_val_x = CLMET_val_full['Text']\n",
    "CLMET_val_y = CLMET_val_full['Year'].astype(int)\n",
    "\n",
    "CLMET_test_x = CLMET_test_full['Text']\n",
    "CLMET_test_y = CLMET_test_full['Year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHER_train_full = pd.read_csv('/Volumes/Korpora/Train/ARCHER_train_tokenized.csv', sep=';')\n",
    "ARCHER_val_full = pd.read_csv('/Volumes/Korpora/Val/ARCHER_val_tokenized.csv', sep=';')\n",
    "ARCHER_test_full = pd.read_csv('/Volumes/Korpora/Test/ARCHER_test_tokenized.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHER_train_full = ARCHER_train_full[(ARCHER_train_full.Year.str.len()== 4) & (ARCHER_train_full.Year.str.isnumeric())]\n",
    "\n",
    "ARCHER_val_full = ARCHER_val_full[(ARCHER_val_full.Year.str.len()== 4) & (ARCHER_val_full.Year.str.isnumeric())]\n",
    "\n",
    "ARCHER_test_full = ARCHER_test_full[(ARCHER_test_full.Year.str.len()== 4) & (ARCHER_test_full.Year.str.isnumeric())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train set:  1049\n",
      "Length validation set:  264\n",
      "Length test set:  329\n"
     ]
    }
   ],
   "source": [
    "print('Length train set: ',len(ARCHER_train_full))\n",
    "print('Length validation set: ', len(ARCHER_val_full))\n",
    "print('Length test set: ', len(ARCHER_test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHER_train_x = ARCHER_train_full['Text']\n",
    "ARCHER_train_y = ARCHER_train_full['Year'].astype(int)\n",
    "\n",
    "ARCHER_val_x = ARCHER_val_full['Text']\n",
    "ARCHER_val_y = ARCHER_val_full['Year'].astype(int)\n",
    "\n",
    "ARCHER_test_x = ARCHER_test_full['Text']\n",
    "ARCHER_test_y = ARCHER_test_full['Year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GERMANC_train_full = pd.read_csv('/Volumes/Korpora/Train/GERMANC_train_tokenized.csv', sep=';')\n",
    "GERMANC_val_full = pd.read_csv('/Volumes/Korpora/Val/GERMANC_val_tokenized.csv', sep=';')\n",
    "GERMANC_test_full = pd.read_csv('/Volumes/Korpora/Test/GERMANC_test_tokenized.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GERMANC_train_full = GERMANC_train_full[(GERMANC_train_full.Year.str.len()== 4) & (GERMANC_train_full.Year.str.isnumeric())]\n",
    "\n",
    "GERMANC_val_full = GERMANC_val_full[(GERMANC_val_full.Year.str.len()== 4) & (GERMANC_val_full.Year.str.isnumeric())]\n",
    "\n",
    "GERMANC_test_full = GERMANC_test_full[(GERMANC_test_full.Year.str.len()== 4) & (GERMANC_test_full.Year.str.isnumeric())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train set:  177\n",
      "Length validation set:  40\n",
      "Length test set:  56\n"
     ]
    }
   ],
   "source": [
    "print('Length train set: ',len(GERMANC_train_full))\n",
    "print('Length validation set: ', len(GERMANC_val_full))\n",
    "print('Length test set: ', len(GERMANC_test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GERMANC_train_x = GERMANC_train_full['Text']\n",
    "GERMANC_train_y = GERMANC_train_full['Year'].astype(int)\n",
    "\n",
    "GERMANC_val_x = GERMANC_val_full['Text']\n",
    "GERMANC_val_y = GERMANC_val_full['Year'].astype(int)\n",
    "\n",
    "GERMANC_test_x = GERMANC_test_full['Text']\n",
    "GERMANC_test_y = GERMANC_test_full['Year'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifier trained on DTA, and validated on GERMANC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_4 = Pipeline([ ('unigram_vectorizer', CountVectorizer(tokenizer=tokenizer_word, min_df = 800)),\n",
    "                    ('feature_selector', SelectKBest(f_regression, k='all')),\n",
    "                         ('ridge_reg', linear_model.Ridge())\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('unigram_vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=800,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_word at 0x1ec13c680>,\n",
       "                                 vocabulary=None)),\n",
       "                ('feature_selector',\n",
       "                 SelectKBest(k='all',\n",
       "                             score_func=<function f_regression at 0x117096a70>)),\n",
       "                ('ridge_reg',\n",
       "                 Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n",
       "                       max_iter=None, normalize=False, random_state=None,\n",
       "                       solver='auto', tol=0.001))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_4.fit(DTA_train_x, DTA_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3700.6021756572445"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_val = reg_4.predict(GERMANC_val_x)\n",
    "mean_squared_error(GERMANC_val_y, y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3565.0475176619693"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = reg_4.predict(GERMANC_test_x)\n",
    "mean_squared_error(GERMANC_test_y, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_val = pd.DataFrame(y_pred_val, columns=['Predicted_y'])\n",
    "\n",
    "diff_pred_true_val = pd.concat([y_pred_val, GERMANC_val_y], axis=1)\n",
    "\n",
    "diff_pred_true_val['Difference'] = diff_pred_true_val.Predicted_y - diff_pred_true_val.Year\n",
    "\n",
    "\n",
    "y_pred_test = pd.DataFrame(y_pred_test, columns=['Predicted_y'])\n",
    "\n",
    "diff_pred_true_test = pd.concat([y_pred_test, GERMANC_test_y], axis=1)\n",
    "\n",
    "diff_pred_true_test['Difference'] = diff_pred_true_test.Predicted_y - diff_pred_true_test.Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diff_pred_true_val.to_csv('/Volumes/Korpora/DTA_over_GERMANC_Exp1b_Reg4_Labels_val.csv',sep=';')\n",
    "diff_pred_true_test.to_csv('/Volumes/Korpora/DTA_over_GERMANC_Exp1b_Reg4_Labels_test.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = reg_4['feature_selector'].get_support()\n",
    "feature_names = reg_4['unigram_vectorizer'].get_feature_names()\n",
    "\n",
    "features_selected = features_to_names(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_details = collect_predictions(GERMANC_val_x, reg_4['ridge_reg'],reg_4['unigram_vectorizer'],features_selected, reg_4)\n",
    "test_details = collect_predictions(GERMANC_test_x, reg_4['ridge_reg'],reg_4['unigram_vectorizer'],features_selected, reg_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_details.to_csv('/Volumes/Korpora/DTA_over_GERMANC_Exp1b_Reg4_Val_results.csv', sep=';')\n",
    "test_details.to_csv('/Volumes/Korpora/DTA_over_GERMANC_Exp1b_Reg4_Test_results.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifier trained on GERMANC, and validated on DTA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_4 = Pipeline([ ('unigram_vectorizer', CountVectorizer(tokenizer=tokenizer_word, min_df = 105)),\n",
    "                    ('feature_selector', SelectKBest(f_regression, k=44)),\n",
    "                         ('ridge_reg', linear_model.Ridge())\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('unigram_vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=105,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_word at 0x1ec13c680>,\n",
       "                                 vocabulary=None)),\n",
       "                ('feature_selector',\n",
       "                 SelectKBest(k=44,\n",
       "                             score_func=<function f_regression at 0x117096a70>)),\n",
       "                ('ridge_reg',\n",
       "                 Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n",
       "                       max_iter=None, normalize=False, random_state=None,\n",
       "                       solver='auto', tol=0.001))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_4.fit(GERMANC_train_x, GERMANC_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8400009.856587972"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_val = reg_4.predict(DTA_val_x)\n",
    "mean_squared_error(DTA_val_y, y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6689227.439661223"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = reg_4.predict(DTA_test_x)\n",
    "mean_squared_error(DTA_test_y, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = pd.DataFrame(y_pred_val, columns=['Predicted_y'])\n",
    "\n",
    "diff_pred_true_val = pd.concat([y_pred_val, DTA_val_y], axis=1)\n",
    "\n",
    "diff_pred_true_val['Difference'] = diff_pred_true_val.Predicted_y - diff_pred_true_val.Publication_year\n",
    "\n",
    "\n",
    "y_pred_test = pd.DataFrame(y_pred_test, columns=['Predicted_y'])\n",
    "\n",
    "diff_pred_true_test = pd.concat([y_pred_test, DTA_test_y], axis=1)\n",
    "\n",
    "diff_pred_true_test['Difference'] = diff_pred_true_test.Predicted_y - diff_pred_true_test.Publication_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diff_pred_true_val.to_csv('/Volumes/Korpora/GERMANC_over_DTA_Exp1b_Reg4_Labels_val.csv',sep=';')\n",
    "diff_pred_true_test.to_csv('/Volumes/Korpora/GERMANC_over_DTA_Exp1b_Reg4_Labels_test.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = reg_4['feature_selector'].get_support()\n",
    "feature_names = reg_4['unigram_vectorizer'].get_feature_names()\n",
    "\n",
    "features_selected = features_to_names(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenizer_word, vocabulary=features_selected) \n",
    "#ELI5 cant't include both vectorizer and feature selector, so this is the best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_details = collect_predictions(DTA_val_x, reg_4['ridge_reg'],vectorizer,features_selected, reg_4)\n",
    "test_details = collect_predictions(DTA_test_x, reg_4['ridge_reg'],vectorizer,features_selected, reg_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_details.to_csv('/Volumes/Korpora/GERMANC_over_DTA_Exp1b_Reg4_Val_results.csv', sep=';')\n",
    "test_details.to_csv('/Volumes/Korpora/GERMANC_over_DTA_Exp1b_Reg4_Test_results.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifier trained on ARCHER, and validated over CLMET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_4 = Pipeline([ ('unigram_vectorizer', CountVectorizer(tokenizer=tokenizer_word, min_df = 500)),\n",
    "                    ('feature_selector', SelectKBest(f_regression, k='all')),\n",
    "                         ('ridge_reg', linear_model.Ridge())\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('unigram_vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=500,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_word at 0x1ec13c680>,\n",
       "                                 vocabulary=None)),\n",
       "                ('feature_selector',\n",
       "                 SelectKBest(k='all',\n",
       "                             score_func=<function f_regression at 0x117096a70>)),\n",
       "                ('ridge_reg',\n",
       "                 Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n",
       "                       max_iter=None, normalize=False, random_state=None,\n",
       "                       solver='auto', tol=0.001))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_4.fit(ARCHER_train_x, ARCHER_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6437126.770897349"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_val = reg_4.predict(CLMET_val_x)\n",
    "mean_squared_error(CLMET_val_y, y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21896163.78334434"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = reg_4.predict(CLMET_test_x)\n",
    "mean_squared_error(CLMET_test_y, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = pd.DataFrame(y_pred_val, columns=['Predicted_y'])\n",
    "\n",
    "diff_pred_true_val = pd.concat([y_pred_val, CLMET_val_y], axis=1)\n",
    "\n",
    "diff_pred_true_val['Difference'] = diff_pred_true_val.Predicted_y - diff_pred_true_val.Year\n",
    "\n",
    "\n",
    "y_pred_test = pd.DataFrame(y_pred_test, columns=['Predicted_y'])\n",
    "\n",
    "diff_pred_true_test = pd.concat([y_pred_test, CLMET_test_y], axis=1)\n",
    "\n",
    "diff_pred_true_test['Difference'] = diff_pred_true_test.Predicted_y - diff_pred_true_test.Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_pred_true_val.to_csv('/Volumes/Korpora/ARCHER_over_CLMET_Exp1b_Reg4_Labels_val.csv',sep=';')\n",
    "diff_pred_true_test.to_csv('/Volumes/Korpora/ARCHER_over_CLMET_Exp1b_Reg4_Labels_test.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = reg_4['feature_selector'].get_support()\n",
    "feature_names = reg_4['unigram_vectorizer'].get_feature_names()\n",
    "\n",
    "features_selected = features_to_names(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_details = collect_predictions(ARCHER_val_x, reg_4['ridge_reg'],reg_4['unigram_vectorizer'],features_selected, reg_4)\n",
    "test_details = collect_predictions(ARCHER_test_x, reg_4['ridge_reg'],reg_4['unigram_vectorizer'],features_selected, reg_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_details.to_csv('/Volumes/Korpora/ARCHER_over_CLMET_Exp1b_Reg4_Val_results.csv', sep=';')\n",
    "test_details.to_csv('/Volumes/Korpora/ARCHER_over_CLMET_Exp1b_Reg4_Test_results.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifier trained on CLMET, and validated over ARCHER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_4 = Pipeline([ ('unigram_vectorizer', CountVectorizer(tokenizer=tokenizer_word, min_df = 167)),\n",
    "                    ('feature_selector', SelectKBest(f_regression, k=40)),\n",
    "                         ('ridge_reg', linear_model.Ridge())\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('unigram_vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=167,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_word at 0x1ec13c680>,\n",
       "                                 vocabulary=None)),\n",
       "                ('feature_selector',\n",
       "                 SelectKBest(k=40,\n",
       "                             score_func=<function f_regression at 0x117096a70>)),\n",
       "                ('ridge_reg',\n",
       "                 Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n",
       "                       max_iter=None, normalize=False, random_state=None,\n",
       "                       solver='auto', tol=0.001))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_4.fit(CLMET_train_x, CLMET_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10212.139003296703"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_val = reg_4.predict(ARCHER_val_x)\n",
    "mean_squared_error(ARCHER_val_y, y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9784.281718876851"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = reg_4.predict(ARCHER_test_x)\n",
    "mean_squared_error(ARCHER_test_y, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = pd.DataFrame(y_pred_val, columns=['Predicted_y'])\n",
    "\n",
    "diff_pred_true_val = pd.concat([y_pred_val, ARCHER_val_y], axis=1)\n",
    "\n",
    "diff_pred_true_val['Difference'] = diff_pred_true_val.Predicted_y - diff_pred_true_val.Year\n",
    "\n",
    "\n",
    "y_pred_test = pd.DataFrame(y_pred_test, columns=['Predicted_y'])\n",
    "\n",
    "diff_pred_true_test = pd.concat([y_pred_test, ARCHER_test_y], axis=1)\n",
    "\n",
    "diff_pred_true_test['Difference'] = diff_pred_true_test.Predicted_y - diff_pred_true_test.Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_pred_true_val.to_csv('/Volumes/Korpora/CLMET_over_ARCHER_Exp1b_Reg4_Labels_val.csv',sep=';')\n",
    "diff_pred_true_test.to_csv('/Volumes/Korpora/CLMET_over_ARCHER_Exp1b_Reg4_Labels_test.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = reg_4['feature_selector'].get_support()\n",
    "feature_names = reg_4['unigram_vectorizer'].get_feature_names()\n",
    "\n",
    "features_selected = features_to_names(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenizer_word, vocabulary=features_selected) \n",
    "#ELI5 cant't include both vectorizer and feature selector, so this is the best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_details = collect_predictions(ARCHER_val_x, reg_4['ridge_reg'],vectorizer,features_selected, reg_4)\n",
    "test_details = collect_predictions(ARCHER_test_x, reg_4['ridge_reg'],vectorizer,features_selected, reg_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_details.to_csv('/Volumes/Korpora/CLMET_over_ARCHER_Exp1b_Reg4_Val_results.csv', sep=';')\n",
    "test_details.to_csv('/Volumes/Korpora/CLMET_over_ARCHER_Exp1b_Reg4_Test_results.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bitb683183fb75b4e14be8d676f44524cdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
