{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 2: DTA, polynomial regression and selecting features that occur a certain amount of time**\n",
    "\n",
    "*Background*: Experiment 2 showed that only using features which occur in 800 out of 899 texts solves the problem of overfitting. The analysis showed that the linear regression does fit well on texts that were written around the middle of the timespan, but it doesn't fit well on texts that were written either very early or very recently. This is surprising because more texts were written in the second half of the timespan than in the first half.\n",
    "\n",
    "*Goal*: Determine if it is possible to predict the year in which a text was written using regression.\n",
    "\n",
    "*Strategies*:\n",
    "\n",
    " - Train on some features that occur a certain amount of time with polynomial regression\n",
    "\n",
    "*Relevance*:\n",
    "\n",
    " If this experiment works, it is possible to estimate years for corpora that have NA's in this variable.\n",
    "\n",
    "*Success criteria*:\n",
    "\n",
    " - Consistent findings over training-, test- and validation set\n",
    " - predicted year is not more than ten years away from the true year\n",
    " \n",
    "*Corpora*:\n",
    "\n",
    " - DTA\n",
    "\n",
    "*Baseline to beat (Exp. 2)*:\n",
    "- Train MSE = 2259.8\n",
    "- Val MSE = 3202.51\n",
    "\n",
    "*Result*: Quadratic polynomial regression performs worse than linear regression, and polynomial regression with a degree of 5 cannot be computed due to the number of polynomial features created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dianaenggist/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/dianaenggist/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest , f_regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.utils\n",
    "import re\n",
    "\n",
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code example: https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le\n",
    "def features_to_names(features, feature_names):\n",
    "    features_selected = []\n",
    "\n",
    "    for bool, feature in zip(features, feature_names):\n",
    "        if bool:\n",
    "            features_selected.append(feature)\n",
    "    return features_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.read_csv('/Volumes/Korpora/Train/DTA_train_tokenized.csv', sep=';')\n",
    "val_full = pd.read_csv('/Volumes/Korpora/Val/DTA_val_tokenized.csv', sep=';')\n",
    "#test_full = pd.read_csv('/Volumes/Korpora/Test/DTA_test_tokenized.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train set:  899\n",
      "Length validation set:  225\n"
     ]
    }
   ],
   "source": [
    "print('Length train set: ',len(train_full))\n",
    "print('Length validation set: ', len(val_full))\n",
    "#print('Length test set: ', len(test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build tokenizer that just substitutes '[' and ']' with ','\n",
    "def tokenizer_word(doc):\n",
    "    doc = re.sub('[(\\[+)|(\\]+)]', '', doc)\n",
    "    doc = re.split(',', doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for assembling predictions in order to find out how features are weighted\n",
    "\n",
    "def collect_predictions(dataset, classifier,vectorizer, feature_names, pipeline):\n",
    "    predictions = eli5.explain_weights_df(classifier,vec=vectorizer, feature_names=feature_names)\n",
    "    \n",
    "    predictions = predictions.drop(['target'], axis=1)\n",
    "    \n",
    "    \n",
    "    #features = predictions['feature']\n",
    "    #predictions = predictions.sort_values('feature')\n",
    "    \n",
    "    \n",
    "    #predictions = predictions.pivot(index='instance',columns='feature', values='weight')\n",
    "    \n",
    "    predictions['YEAR'] = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    for instance in range (0, len(dataset)):\n",
    "        pred = eli5.explain_prediction_df(classifier, dataset[instance], vec=vectorizer, feature_names=feature_names)\n",
    "        source_text = pd.DataFrame([[dataset[instance]]])\n",
    "        year_pred = pipeline.predict(source_text[0])\n",
    "        pred['weight_value'] = pred['weight'] * pred['value']\n",
    "        pred['instance'] = instance\n",
    "        \n",
    "        \n",
    "        pred = pred.drop(['target','weight','value'], axis=1)\n",
    "        \n",
    "        #pred = pred.merge(features, how='right', on='feature')\n",
    "        \n",
    "        #pred = pred.sort_values('feature')\n",
    "        #pred = pred.pivot(index = 'instance', columns='feature', values = 'weight_value')\n",
    "        \n",
    "        pred['YEAR'] = np.round(year_pred[0])\n",
    "    \n",
    "        predictions = pd.concat([predictions, pred])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_full['Text']\n",
    "train_y = train_full['Publication_year']\n",
    "\n",
    "val_x = val_full['Text']\n",
    "val_y = val_full['Publication_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Regression with sklearn: https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_1 = Pipeline([ ('unigram_vectorizer', CountVectorizer(tokenizer=tokenizer_word, min_df = 800)),\n",
    "                    ('feature_selector', SelectKBest(f_regression, k='all')),\n",
    "                    ('poly', PolynomialFeatures(degree=2)),\n",
    "                    ('linear', linear_model.LinearRegression(fit_intercept=False))\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('unigram_vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=800,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_word at 0x1da54dc20>,\n",
       "                                 vocabulary=None)),\n",
       "                ('feature_selector',\n",
       "                 SelectKBest(k='all',\n",
       "                             score_func=<function f_regression at 0x11fb51170>)),\n",
       "                ('poly',\n",
       "                 PolynomialFeatures(degree=2, include_bias=True,\n",
       "                                    interaction_only=False, order='C')),\n",
       "                ('linear',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=False, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_1.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158045.95707092748"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = reg_1.predict(train_x)\n",
    "mean_squared_error(train_y, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21964810465.566597"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_val = reg_1.predict(val_x)\n",
    "\n",
    "mean_squared_error(val_y, y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg_2 = Pipeline([ ('unigram_vectorizer', CountVectorizer(tokenizer=tokenizer_word, min_df = 800)),\n",
    "                    #('feature_selector', SelectKBest(f_regression, k='all')),\n",
    "                    #('poly', PolynomialFeatures(degree=5)),\n",
    "                    #('linear', linear_model.LinearRegression(fit_intercept=False))\n",
    "                        #])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reg_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7db527dc2da0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreg_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reg_2' is not defined"
     ]
    }
   ],
   "source": [
    "#reg_2.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "215 features with degree = 5 ends up with 4'102'565'544 features, and the kernel can't handle this. Therefore, the next step is to reduce the number of features, because quadratic regression performs worse than linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = reg_2.predict(train_x)\n",
    "mean_squared_error(train_y, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = reg_2.predict(val_x)\n",
    "\n",
    "mean_squared_error(val_y, y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names has a wrong length: expected=23436, got=215",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-05086dcfed68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeatures_selected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_to_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0meli5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unigram_vectorizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/R_python/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m                             '1 positional argument')\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/eli5/sklearn/explain_weights.py\u001b[0m in \u001b[0;36mexplain_linear_regressor_weights\u001b[0;34m(reg, vec, top, target_names, targets, feature_names, coef_scale, feature_re, feature_filter)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mfeature_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mfeature_re\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_re\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m     \u001b[0m_extra_caveats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mHASHING_CAVEATS\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_invhashing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/eli5/sklearn/utils.py\u001b[0m in \u001b[0;36mget_feature_names_filtered\u001b[0;34m(clf, vec, bias_name, feature_names, num_features, feature_filter, feature_re, estimator_feature_names)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mestimator_feature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator_feature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/eli5/sklearn/utils.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(clf, vec, bias_name, feature_names, num_features, estimator_feature_names)\u001b[0m\n\u001b[1;32m    109\u001b[0m             raise ValueError(\"feature_names has a wrong length: \"\n\u001b[1;32m    110\u001b[0m                              \"expected=%d, got=%d\" % (num_features,\n\u001b[0;32m--> 111\u001b[0;31m                                                       len(feature_names)))\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mFeatureNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names has a wrong length: expected=23436, got=215"
     ]
    }
   ],
   "source": [
    "features = reg_1['feature_selector'].get_support()\n",
    "feature_names = reg_1['unigram_vectorizer'].get_feature_names()\n",
    "\n",
    "features_selected = features_to_names(features, feature_names)\n",
    "\n",
    "eli5.explain_weights(reg_1['linear'],vec=reg_1['unigram_vectorizer'], feature_names=features_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Predicted_y  Publication_year  Difference\n",
      "0  1821.832534              1741   80.832534\n",
      "1  1718.854585              1691   27.854585\n",
      "2  1817.362107              1665  152.362107\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = pd.DataFrame(y_pred_train, columns=['Predicted_y'])\n",
    "\n",
    "diff_pred_true_train = pd.concat([y_pred_train, train_y], axis=1)\n",
    "\n",
    "diff_pred_true_train['Difference'] = diff_pred_true_train.Predicted_y - diff_pred_true_train.Publication_year\n",
    "    \n",
    "\n",
    "print(diff_pred_true_train.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Predicted_y  Publication_year     Difference\n",
      "0   -9757.105495              1897  -11654.105495\n",
      "1  146451.113027              1701  144750.113027\n",
      "2  109784.712197              1663  108121.712197\n"
     ]
    }
   ],
   "source": [
    "y_pred_val = pd.DataFrame(y_pred_val, columns=['Predicted_y'])\n",
    "\n",
    "diff_pred_true_val = pd.concat([y_pred_val, val_y], axis=1)\n",
    "\n",
    "diff_pred_true_val['Difference'] = diff_pred_true_val.Predicted_y - diff_pred_true_val.Publication_year\n",
    "\n",
    "print(diff_pred_true_val.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_pred_true_val.to_csv('/Volumes/Korpora/Exp3_Reg1_Labels_val.csv',sep=';')\n",
    "diff_pred_true_train.to_csv('/Volumes/Korpora/Exp3_Reg1_Labels_train.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names has a wrong length: expected=23436, got=215",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-178ea24ac6e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unigram_vectorizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_details\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Volumes/Korpora/Exp3_Reg1_Val_results.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9c269be9b9f0>\u001b[0m in \u001b[0;36mcollect_predictions\u001b[0;34m(dataset, classifier, vectorizer, feature_names, pipeline)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollect_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_weights_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/eli5/formatters/as_dataframe.py\u001b[0m in \u001b[0;36mexplain_weights_df\u001b[0;34m(estimator, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     return format_as_dataframe(\n\u001b[0;32m---> 23\u001b[0;31m         eli5.explain_weights(estimator, **kwargs))\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/R_python/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m                             '1 positional argument')\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/eli5/sklearn/explain_weights.py\u001b[0m in \u001b[0;36mexplain_linear_regressor_weights\u001b[0;34m(reg, vec, top, target_names, targets, feature_names, coef_scale, feature_re, feature_filter)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mfeature_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mfeature_re\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_re\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m     \u001b[0m_extra_caveats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mHASHING_CAVEATS\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_invhashing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/eli5/sklearn/utils.py\u001b[0m in \u001b[0;36mget_feature_names_filtered\u001b[0;34m(clf, vec, bias_name, feature_names, num_features, feature_filter, feature_re, estimator_feature_names)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mestimator_feature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator_feature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/eli5/sklearn/utils.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(clf, vec, bias_name, feature_names, num_features, estimator_feature_names)\u001b[0m\n\u001b[1;32m    109\u001b[0m             raise ValueError(\"feature_names has a wrong length: \"\n\u001b[1;32m    110\u001b[0m                              \"expected=%d, got=%d\" % (num_features,\n\u001b[0;32m--> 111\u001b[0;31m                                                       len(feature_names)))\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mFeatureNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names has a wrong length: expected=23436, got=215"
     ]
    }
   ],
   "source": [
    "val_details = collect_predictions(val_x, reg_1['linear'],reg_1['unigram_vectorizer'],features_selected, reg_1)\n",
    "val_details.to_csv('/Volumes/Korpora/Exp3_Reg1_Val_results.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_details = collect_predictions(train_x, reg_1['linear'],reg_1['unigram_vectorizer'],features_selected, reg_1)\n",
    "train_details.to_csv('/Volumes/Korpora/Exp3_Reg1_train_results.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
