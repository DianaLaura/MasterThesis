{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 4: DTA, linear regression and subwords**\n",
    "\n",
    "*Background*: Experiment 2 shows that linear regression with 215 features does not overfit anymore, but the model does not fit the beginning and the end of the timerange. Quadratic polynomial regression would be a possibility, but quadratic polynomial regression performs worse on both training and validation set than linear regression. Polynomial regression with a degree of 5 cannot be computed due to the vast number of polynomial features created by the model.\n",
    "\n",
    "*Goal*: Determine if it is possible to predict the year in which a text was written using regression.\n",
    "\n",
    "*Strategies*:\n",
    "\n",
    "- Use a BPE-transformer to train on subwords (Sennrich2016)\n",
    "\n",
    "*Relevance*:\n",
    "\n",
    "- If this experiment works, it is possible to estimate years for corpora that have NA's in this variable.\n",
    "- Subwords might increase the amount of generalization, and minimize the vocabulary used at the same time.\n",
    "\n",
    "*Success criteria*:\n",
    "\n",
    "- Consistent findings over training-, test- and validation set\n",
    "- predicted year is not more than ten years away from the true year\n",
    "\n",
    "*Corpora*:\n",
    "\n",
    "- DTA\n",
    "\n",
    "*Baseline to beat (Exp. 2)*:\n",
    "- Train MSE = 2259.8\n",
    "- Val MSE = 3202.51\n",
    "\n",
    "*Result*: - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dianaenggist/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/dianaenggist/opt/anaconda3/envs/R_python/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest , f_regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.utils\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from Selfwritten_modules.SubwordTransformer import SubwordTransformer\n",
    "\n",
    "import re\n",
    "\n",
    "import eli5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test bpe-algorithm on a small play-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "playset = pd.read_csv('/users/dianaenggist/Documents/Masterprojekt/testfile_deutsch_tokenized.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               Text\n",
      "0           0  [['Die', 'Pressekonferenz', 'ist', 'beendet', ...\n",
      "1           1  [['Strupler', ':', '«', 'Wir', 'müssen', 'zuer...\n",
      "2           2  [['Koch', ':', '«', 'Es', 'gibt', 'bereits', '...\n"
     ]
    }
   ],
   "source": [
    "print(playset)\n",
    "playset = playset['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build tokenizer that just substitutes '[' and ']' with ','\n",
    "def tokenizer_word(doc):\n",
    "    doc = re.sub('[(\\[+)|(\\]+)]', '', doc)\n",
    "    doc = re.split(',', doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = SubwordTransformer(tokenizer=tokenizer_word)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e,n\n",
      "en\n",
      "e,n\n",
      "en\n",
      "e,r\n",
      "er\n",
      "e,r\n",
      "er\n",
      "o,n\n",
      "on\n",
      "o,n\n",
      "on\n",
      "a,n\n",
      "an\n",
      "a,n\n",
      "an\n",
      "e,i\n",
      "ei\n",
      "e,i\n",
      "ei\n",
      "i,e\n",
      "ie\n",
      "i,e\n",
      "ie\n",
      "e,s\n",
      "es\n",
      "e,s\n",
      "es\n",
      "ei,t\n",
      "eit\n",
      "ei,t\n",
      "eit\n",
      "u,n\n",
      "un\n",
      "u,n\n",
      "un\n",
      "c,h\n",
      "ch\n",
      "c,h\n",
      "ch\n"
     ]
    }
   ],
   "source": [
    "vocabulary = transformer.bpe(playset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'D,ie': 2, 'P,r,es,s,e,k,on,f,er,en,z': 1, 'i,s,t': 1, 'b,e,en,d,e,t': 1, '.': 6, 'V,ie,l,en': 1, 'D,an,k': 1, 'f,ü,r': 1, 'I,h,r,e': 1, 'A,u,f,m,er,k,s,a,m,k,eit': 1, 'W,eit,er': 1, 'g,e,h,t': 1, 'es': 1, 'u,m': 2, '1,5,.,3,0': 1, 'U,h,r': 1, 'm,i,t': 1, 'd,er': 1, 'M,e,d,ien,k,on,f,er,en,z': 1, 'd,es': 1, 'K,an,t,on,s': 1, 'G,r,a,u,b,ü,n,d,en': 1, 'B,e,h,ö,r,d,en': 1, 'n,e,h,m,en': 1, 'S,t,e,l,l,un,g': 1, 'z,u': 2, 'd,en': 1, 'z,w,ei': 1, 'b,es,t,ä,t,i,g,t,en': 1, 'C,o,r,on,a,v,i,r,u,s,-,F,ä,l,l,en': 1, 'i,m': 1, 'K,an,t,on': 1, 'K,o,ch': 1, ':': 1, '«': 1, 'E,s': 2, 'g,i,b,t': 1, 'b,er,eit,s': 1, 'e,u,r,o,p,a,w,eit': 1, 'A,u,f,r,u,f,e': 1, \"'\": 6, 'F,o,r,s,ch,un,g': 1, 'g,en,er,ier,en': 1, 'w,i,r,d': 1, 's,ch,n,e,l,l': 1, 'un,d': 2, 's,t,a,r,k': 1, 'an': 2, 'I,m,p,f,s,t,o,f,f,en': 1, 'M,e,d,i,k,a,m,en,t,en': 1, 'g,e,f,o,r,s,ch,t,.': 1, '»': 1, 'Z,u': 1, 's,a,g,en': 1, 'w,ie': 1, 'l,an,g,e': 1, 'd,a,s': 1, 'd,a,u,er,e': 1, 's,ei': 1, 'a,b,er': 1, 'S,p,e,k,u,l,a,t,i,on': 1})\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code example: https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le\n",
    "def features_to_names(features, feature_names):\n",
    "    features_selected = []\n",
    "\n",
    "    for bool, feature in zip(features, feature_names):\n",
    "        if bool:\n",
    "            features_selected.append(feature)\n",
    "    return features_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for assembling predictions in order to find out how features are weighted\n",
    "\n",
    "def collect_predictions(dataset, classifier,vectorizer, feature_names, pipeline):\n",
    "    predictions = eli5.explain_weights_df(classifier,vec=vectorizer, feature_names=feature_names)\n",
    "    \n",
    "    predictions = predictions.drop(['target'], axis=1)\n",
    "    \n",
    "    \n",
    "    predictions['YEAR'] = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    for instance in range (0, len(dataset)):\n",
    "        pred = eli5.explain_prediction_df(classifier, dataset[instance], vec=vectorizer, feature_names=feature_names)\n",
    "        source_text = pd.DataFrame([[dataset[instance]]])\n",
    "        year_pred = pipeline.predict(source_text[0])\n",
    "        pred['weight_value'] = pred['weight'] * pred['value']\n",
    "        pred['instance'] = instance\n",
    "        \n",
    "        \n",
    "        pred = pred.drop(['target','weight','value'], axis=1)\n",
    "        \n",
    "    \n",
    "        pred['YEAR'] = np.round(year_pred[0])\n",
    "    \n",
    "        predictions = pd.concat([predictions, pred])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.read_csv('/Volumes/Korpora/Train/DTA_train_tokenized.csv', sep=';')\n",
    "val_full = pd.read_csv('/Volumes/Korpora/Val/DTA_val_tokenized.csv', sep=';')\n",
    "#test_full = pd.read_csv('/Volumes/Korpora/Test/DTA_test_tokenized.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length train set: ',len(train_full))\n",
    "print('Length validation set: ', len(val_full))\n",
    "#print('Length test set: ', len(test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_full['Text']\n",
    "train_y = train_full['Publication_year']\n",
    "\n",
    "val_x = val_full['Text']\n",
    "val_y = val_full['Publication_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
